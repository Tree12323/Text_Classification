{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b35b4687-4f55-4830-b98f-cec2a1ab8a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuchh/anaconda3/envs/nlp1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/liuchh/anaconda3/envs/nlp1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/liuchh/anaconda3/envs/nlp1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/liuchh/anaconda3/envs/nlp1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/liuchh/anaconda3/envs/nlp1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/liuchh/anaconda3/envs/nlp1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/liuchh/anaconda3/envs/nlp1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/liuchh/anaconda3/envs/nlp1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/liuchh/anaconda3/envs/nlp1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/liuchh/anaconda3/envs/nlp1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/liuchh/anaconda3/envs/nlp1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/liuchh/anaconda3/envs/nlp1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n"
     ]
    }
   ],
   "source": [
    "import datatable as dt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os, gc, random, time\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Bidirectional,\n",
    "    Embedding,\n",
    "    GRU,\n",
    "    GlobalAveragePooling1D,\n",
    "    GlobalMaxPooling1D,\n",
    "    Concatenate,\n",
    "    SpatialDropout1D,\n",
    "    BatchNormalization,\n",
    "    Dropout,\n",
    "    Dense,\n",
    "    Conv1D,\n",
    "    concatenate,\n",
    "    Activation,\n",
    "    Input\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), len(logical_gpus))\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5692e5a9-2fb1-481f-be76-3d959a208ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = dt.fread('/home/liuchh/kaggle/input/train_set.csv', sep='\\t').to_pandas()\n",
    "test_df = dt.fread('/home/liuchh/kaggle/input/test_a.csv', sep='\\t').to_pandas()\n",
    "\n",
    "new_data = np.load('/home/liuchh/kaggle/input/pl_ensemble_0.95.npy')\n",
    "new_data_x = test_df.iloc[new_data[:,0]].text.values\n",
    "new_data_y = new_data[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c65044f3-87de-4d80-a943-ec01b9600d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=7000,\n",
    "    lower=False,\n",
    "    filters=\"\"\n",
    ")\n",
    "tokenizer.fit_on_texts(list(train_df['text'].values) + list(test_df['text'].values))\n",
    "train_ = tokenizer.texts_to_sequences(train_df['text'])\n",
    "test_ = tokenizer.texts_to_sequences(test_df['text'])\n",
    "new_ = tokenizer.texts_to_sequences(new_data_x)\n",
    "train_ = tf.keras.preprocessing.sequence.pad_sequences(train_, maxlen=2400)\n",
    "test_ = tf.keras.preprocessing.sequence.pad_sequences(test_, maxlen=2400)\n",
    "new_ = tf.keras.preprocessing.sequence.pad_sequences(new_,maxlen=2400)\n",
    "word_vocab = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f61a321-021f-4221-9dc3-74fe8ef050c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec ......\n",
      "Add word2vec finished ......\n"
     ]
    }
   ],
   "source": [
    "all_data = pd.concat([train_df['text'], test_df['text']])\n",
    "file_name = '/home/liuchh/kaggle/input/word2vec.bin'\n",
    "if not os.path.exists(file_name):\n",
    "    print('Training Word2Vec ......')\n",
    "    model = Word2Vec(\n",
    "        [[word for word in document.split(' ')] for document in all_data.values],\n",
    "        size=200,\n",
    "        window=5,\n",
    "        iter=10,\n",
    "        workers=12,\n",
    "        seed=2021,\n",
    "        min_count=2\n",
    "    )\n",
    "    model.save(file_name)\n",
    "else:\n",
    "    print('Loading Word2Vec ......')\n",
    "    model = Word2Vec.load(file_name)\n",
    "print('Add word2vec finished ......')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b6bb80c-870c-40a7-b9aa-2b5338daa089",
   "metadata": {},
   "outputs": [],
   "source": [
    "Glove_model = gensim.models.KeyedVectors.load_word2vec_format('/home/liuchh/kaggle/input/Glove_200.txt',binary=False)\n",
    "\n",
    "count = 0\n",
    "embedding_matrix = np.zeros((len(word_vocab) + 1, 400))\n",
    "for word, i in word_vocab.items():\n",
    "    embedding_vector = np.concatenate((model.wv[word],Glove_model[word])) if word in model.wv else None\n",
    "    if embedding_vector is not None:\n",
    "        count += 1\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        unk_vec = np.random.random(400) * 0.5\n",
    "        unk_vec = unk_vec - unk_vec.mean()\n",
    "        embedding_matrix[i] = unk_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3532f7b6-557a-47d5-add7-f9beab4f2b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TextGRU(sent_length, embeddings_weight):\n",
    "    content = Input(shape=(sent_length,), dtype='int32')\n",
    "    embedding = Embedding(\n",
    "        name=\"word_embedding\",\n",
    "        input_dim=embeddings_weight.shape[0],\n",
    "        weights=[embeddings_weight],\n",
    "        output_dim=embeddings_weight.shape[1],\n",
    "        trainable=True)\n",
    "    x = SpatialDropout1D(0.2)(embedding(content))\n",
    "    x = Bidirectional(GRU(400, return_sequences=True))(x)\n",
    "    x = Bidirectional(GRU(400, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    x = Dense(1024)(conc)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation=\"relu\")(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = Dense(512)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation=\"relu\")(x)\n",
    "    output = Dense(14, activation=\"softmax\")(x)\n",
    "    model = tf.keras.models.Model(inputs=content, outputs=output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7771747f-9602-4354-8beb-b094e2c77da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_performance_measure(labels_right, labels_pred):\n",
    "    text_labels = list(set(labels_right))\n",
    "    test_pred_labels = list(set(labels_pred))\n",
    "    \n",
    "    TP = dict.fromkeys(text_labels, 0)\n",
    "    TP_FP = dict.fromkeys(text_labels, 0)\n",
    "    TP_FN = dict.fromkeys(text_labels, 0)\n",
    "    \n",
    "    for i in range(0, len(labels_right)):\n",
    "        TP_FP[labels_right[i]] += 1\n",
    "        TP_FN[labels_right[i]] += 1\n",
    "        if labels_right[i] == labels_pred[i]:\n",
    "            TP[labels_right[i]] += 1\n",
    "        \n",
    "    for key in TP_FP:\n",
    "        P = float(TP[key]) / float(TP_FP[key] + 1)\n",
    "        R = float(TP[key]) / float(TP_FN[key] + 1)\n",
    "        F1 = P * R * 2 / (P + R) if (P + R) != 0 else 0\n",
    "        print(\"%s:\\t P:%f\\t R:%f\\t F1:%f\" % (key,P,R,F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32864911-f818-497d-803c-9cf493861ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=2021)\n",
    "cv_socres = []\n",
    "train_label = train_df['label'].values\n",
    "train_label = to_categorical(train_label)\n",
    "new_data_y = to_categorical(new_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc0ff81-cb42-4f83-9c58-4e7e3dd416e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/liuchh/anaconda3/envs/nlp1/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/liuchh/anaconda3/envs/nlp1/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/liuchh/anaconda3/envs/nlp1/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/liuchh/anaconda3/envs/nlp1/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/liuchh/anaconda3/envs/nlp1/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "with tf.device('/gpu:1'):\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(train_, train_label, shuffle=True, random_state=2021, stratify=train_label)\n",
    "    \n",
    "    X_train, X_valid = tf.concat([X_train,new_],axis=0), X_valid\n",
    "    y_train, y_valid = np.append(y_train,new_data_y,axis=0), y_valid\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(10000).batch(512)\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((X_valid, y_valid)).batch(512)\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((test_, np.zeros((test_.shape[0], 14)))).batch(512)\n",
    "\n",
    "    checkpoint_dir = './TextGRU_400_cv_finetune_checkpoints/cv_'+str(i)+'/'\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "    model = TextGRU(2400, embedding_matrix)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "    plateau = ReduceLROnPlateau(monitor='val_accuracy', verbose=1, mode='max', factor=0.5, patience=3)\n",
    "    checkpoint = ModelCheckpoint(checkpoint_prefix, monitor='val_accuracy', verbose=2, save_best_only=True, mode='max', save_weights_only=True)\n",
    "    model.fit(\n",
    "            train_ds,\n",
    "            epochs=10,\n",
    "            validation_data=val_ds,\n",
    "            callbacks=[early_stopping, plateau, checkpoint],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "    valid_prob = model.predict(val_ds)\n",
    "    valid_pred = np.argmax(valid_prob,axis=1)\n",
    "    y_valid = np.argmax(y_valid, axis=1)\n",
    "\n",
    "    f1_score_ = f1_score(y_valid,valid_pred,average='macro') \n",
    "    print (\"valid's f1-score: %s\" %f1_score_)\n",
    "\n",
    "    test_pre_matrix = model.predict(test_ds)\n",
    "\n",
    "    del model; gc.collect()\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69234ad-bd9c-43b3-892c-2c898fc9b159",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"TextGRU_400finetune_test_result.npy\",test_pre_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
