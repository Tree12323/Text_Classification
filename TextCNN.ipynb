{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111b9856-11fb-49e2-a660-a6ca89e54499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datatable as dt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os, gc, random, time\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.layers import (\n",
    "    Bidirectional,\n",
    "    Embedding,\n",
    "    GlobalAveragePooling1D,\n",
    "    GlobalMaxPooling1D,\n",
    "    Concatenate,\n",
    "    SpatialDropout1D,\n",
    "    BatchNormalization,\n",
    "    Dropout,\n",
    "    Dense,\n",
    "    Conv1D,\n",
    "    Activation,\n",
    "    Input\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), len(logical_gpus))\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e914b1-948c-4d6e-baef-73da94eea89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = dt.fread('/home/liuchh/kaggle/input/train_set.csv', sep='\\t').to_pandas()\n",
    "test_df = dt.fread('/home/liuchh/kaggle/input/test_a.csv', sep='\\t').to_pandas()\n",
    "\n",
    "new_data = np.load('/home/liuchh/kaggle/input/pl_ensemble_0.95.npy')\n",
    "new_data_x = test_df.iloc[new_data[:,0]].text.values\n",
    "new_data_y = new_data[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8686f591-45e4-488d-86c1-9c1020d4a759",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=7000,\n",
    "    lower=False,\n",
    "    filters=\"\"\n",
    ")\n",
    "tokenizer.fit_on_texts(list(train_df['text'].values) + list(test_df['text'].values))\n",
    "train_ = tokenizer.texts_to_sequences(train_df['text'])\n",
    "test_ = tokenizer.texts_to_sequences(test_df['text'])\n",
    "new_ = tokenizer.texts_to_sequences(new_data_x)\n",
    "train_ = tf.keras.preprocessing.sequence.pad_sequences(train_, maxlen=2400)\n",
    "test_ = tf.keras.preprocessing.sequence.pad_sequences(test_, maxlen=2400)\n",
    "new_ = tf.keras.preprocessing.sequence.pad_sequences(new_,maxlen=2400)\n",
    "word_vocab = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe38c3d-7db6-4da4-8355-5c9010724e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train_df['text'], test_df['text']])\n",
    "file_name = '/home/liuchh/kaggle/input/word2vec.bin'\n",
    "if not os.path.exists(file_name):\n",
    "    print('Training Word2Vec ......')\n",
    "    model = Word2Vec(\n",
    "        [[word for word in document.split(' ')] for document in all_data.values],\n",
    "        size=200,\n",
    "        window=5,\n",
    "        iter=10,\n",
    "        workers=12,\n",
    "        seed=2021,\n",
    "        min_count=2\n",
    "    )\n",
    "    model.save(file_name)\n",
    "else:\n",
    "    print('Loading Word2Vec ......')\n",
    "    model = Word2Vec.load(file_name)\n",
    "print('Add word2vec finished ......')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9c032b-ff66-4ac9-aa0a-58ff1b04721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Glove_model = gensim.models.KeyedVectors.load_word2vec_format('/home/liuchh/kaggle/input/Glove_200.txt',binary=False)\n",
    "\n",
    "count = 0\n",
    "embedding_matrix = np.zeros((len(word_vocab) + 1, 400))\n",
    "for word, i in word_vocab.items():\n",
    "    embedding_vector = np.concatenate((model.wv[word],Glove_model[word])) if word in model.wv else None\n",
    "    if embedding_vector is not None:\n",
    "        count += 1\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        unk_vec = np.random.random(400) * 0.5\n",
    "        unk_vec = unk_vec - unk_vec.mean()\n",
    "        embedding_matrix[i] = unk_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc09eefa-6d93-4da6-a27c-3b3b0b953900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TextCNN(sent_length, embeddings_weight):\n",
    "    content = Input(shape=(sent_length,), dtype='int32')\n",
    "    embedding = Embedding(\n",
    "        name=\"word_embedding\",\n",
    "        input_dim=embeddings_weight.shape[0],\n",
    "        weights=[embeddings_weight],\n",
    "        output_dim=embeddings_weight.shape[1],\n",
    "        trainable=True)\n",
    "    x = SpatialDropout1D(0.2)(embedding(content))\n",
    "    convs = []\n",
    "    for kernel_size in [2,3,4,5]:\n",
    "        c = Conv1D(1024, kernel_size, activation='relu')(x)\n",
    "        c = GlobalMaxPooling1D()(c)\n",
    "        convs.append(c)\n",
    "    x = Concatenate()(convs)\n",
    "    x = Dense(512,activation=\"relu\")(x)\n",
    "    output = Dense(14, activation=\"softmax\")(x)\n",
    "    model = tf.keras.models.Model(inputs=content, outputs = output)\n",
    "    model.compile(loss=\"categorical_crossentropy\",optimizer='adam',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67da3249-ff85-4e15-b4ff-a4cd598b13db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_performance_measure(labels_right, labels_pred):\n",
    "    text_labels = list(set(labels_right))\n",
    "    test_pred_labels = list(set(labels_pred))\n",
    "    \n",
    "    TP = dict.fromkeys(text_labels, 0)\n",
    "    TP_FP = dict.fromkeys(text_labels, 0)\n",
    "    TP_FN = dict.fromkeys(text_labels, 0)\n",
    "    \n",
    "    for i in range(0, len(labels_right)):\n",
    "        TP_FP[labels_right[i]] += 1\n",
    "        TP_FN[labels_right[i]] += 1\n",
    "        if labels_right[i] == labels_pred[i]:\n",
    "            TP[labels_right[i]] += 1\n",
    "        \n",
    "    for key in TP_FP:\n",
    "        P = float(TP[key]) / float(TP_FP[key] + 1)\n",
    "        R = float(TP[key]) / float(TP_FN[key] + 1)\n",
    "        F1 = P * R * 2 / (P + R) if (P + R) != 0 else 0\n",
    "        print(\"%s:\\t P:%f\\t R:%f\\t F1:%f\" % (key,P,R,F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c08375b-3444-4351-82c2-fccf21a323b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=2021)\n",
    "\n",
    "train_label = train_df['label'].values\n",
    "train_label = to_categorical(train_label)\n",
    "new_data_y = to_categorical(new_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff46b798-a59f-49a8-b605-2e4266c8fee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "with tf.device('/gpu:1'):\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(train_, train_label, shuffle=True, random_state=2021, stratify=train_label)\n",
    "    \n",
    "#     X_train = train_\n",
    "#     y_train = train_label\n",
    "    \n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(10000).batch(256)\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((X_valid, y_valid)).batch(256)\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((test_, np.zeros((test_.shape[0], 14)))).batch(256)\n",
    "    \n",
    "    checkpoint_dir = './TextCNN_400_cv_finetune_checkpoints/cv_'+str(i)+'/'\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "    \n",
    "    model = TextCNN(2400, embedding_matrix)\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "    plateau = ReduceLROnPlateau(monitor='val_accuracy', verbose=1, mode='max', factor=0.5, patience=3)\n",
    "    checkpoint = ModelCheckpoint(checkpoint_prefix, monitor='val_accuracy', verbose=2, save_best_only=True, mode='max', save_weights_only=True)\n",
    "    \n",
    "    model.fit(\n",
    "        train_ds,\n",
    "        epochs=10,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=[early_stopping, plateau, checkpoint],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    valid_prob = model.predict(val_ds)\n",
    "    valid_pred = np.argmax(valid_prob,axis=1)\n",
    "    y_valid = np.argmax(y_valid, axis=1)\n",
    "\n",
    "    f1_score_ = f1_score(y_valid,valid_pred,average='macro') \n",
    "    print (\"valid's f1-score: %s\" %f1_score_)\n",
    "    \n",
    "    test_pre_matrix = model.predict(test_ds)\n",
    "    \n",
    "    np.save(\"TextCNN_400finetune_test_result_no_cv.npy\", test_pre_matrix)\n",
    "    \n",
    "    model.save('my_model_no_cv.h5')\n",
    "    del model; gc.collect()\n",
    "    tf.keras.backend.clear_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
